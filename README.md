# Проект по детекции логотипа Т-Банка

**Работу выполнил:** Близниченко Алексей, студент 2 курса бакалавриата АНО ВО "Центральный университет", направление "Искусственный интеллект".

_Работа выполнена в рамках отборочного этапа на смену по ML в Сириусе при партнерстве Т-Банка._

<details open>
<summary>Разделы</summary>

<!-- TOC -->
* [Проект по детекции логотипа Т-Банка](#проект-по-детекции-логотипа-т-банка)
* [Установка и запуск](#установка-и-запуск)
  * [Требования](#требования)
  * [Запуск в Docker (рекомендуемый)](#запуск-в-docker-рекомендуемый)
  * [Альтернативный локальный запуск (без Docker)](#альтернативный-локальный-запуск-без-docker)
  * [Переменные окружения](#переменные-окружения)
  * [Эндпойнты и проверка](#эндпойнты-и-проверка)
  * [Частые ошибки и способы решения](#частые-ошибки-и-способы-решения)
* [Подход к решению](#подход-к-решению)
  * [Подготовка и выбор модели](#подготовка-и-выбор-модели)
  * [Обучение модели](#обучение-модели)
  * [Разработка сервиса](#разработка-сервиса)
  * [Возможные улучшения и анализ работы](#возможные-улучшения-и-анализ-работы)
  * [Список использованных инструментов:](#список-использованных-инструментов)
* [Благодарности](#благодарности)
<!-- TOC -->

</details>

# Установка и запуск

Рекомендуемый способ запуска — через Docker; локальный запуск без контейнера предназначен для разработки. По умолчанию сервис слушает порт 8000 и предоставляет HTTP API (FastAPI) с эндпойнтами /health и /detect.

## Требования
- Docker и Docker Compose (v2).
- Git и интернет‑доступ для загрузки образов/зависимостей.
- Файл модели: по умолчанию используется models/model.onnx (YOLOv11n, в репозитории); при необходимости может быть заменён.

## Запуск в Docker (рекомендуемый)
Используется базовый образ python:3.11-slim. Модель монтируется в контейнер в режиме только для чтения.

1. Клонировать репозиторий и перейти в папку проекта:
    - Windows (PowerShell):
      - `git clone https://github.com/ravik-games/Tbank-Logo-Detection.git`
      - `cd Tbank-Logo-Detection`
    - Linux/macOS:
      - `git clone https://github.com/ravik-games/Tbank-Logo-Detection.git`
      - `cd Tbank-Logo-Detection`

2. Собрать образ:
   - `docker build -t tbank-logo-api .`
   - Docker Compose (рекомендуемый способ):
     - `docker compose up -d`
     - По умолчанию публикуется порт 8000; модель берётся из ./models. Переменные окружения настраиваются в docker-compose.yml.
   - Запуск (CPU):
     - Windows (PowerShell):
       - `docker run --rm -p 8000:8000 -v ${PWD}\models:/models:ro -e MODEL_PATH=/models/model.onnx -e DEVICE=cpu tbank-logo-api`
     - Linux/macOS:
       - `docker run --rm -p 8000:8000 -v $(pwd)/models:/models:ro -e MODEL_PATH=/models/model.onnx -e DEVICE=cpu tbank-logo-api`

Примечание по GPU:
- Базовая конфигурация образа использует onnxruntime (CPU) так как модель не требует больших вычислений и быстро работает на CPU.
- Для использования NVIDIA GPU требуется:
  - заменить зависимость onnxruntime на onnxruntime-gpu в requirements.txt и пересобрать образ;
  - указать `DEVICE=cuda:0`;
  - запускать контейнер с поддержкой GPU: 
    - `docker run --gpus all ...`.

## Альтернативный локальный запуск (без Docker)
1. Клонировать репозиторий и перейти в папку проекта:
   - Windows (PowerShell):
     - `git clone https://github.com/ravik-games/Tbank-Logo-Detection.git`
     - `cd Tbank-Logo-Detection`
   - Linux/macOS:
     - `git clone https://github.com/ravik-games/Tbank-Logo-Detection.git`
     - `cd Tbank-Logo-Detection`
2. Создать и активировать виртуальное окружение:
   - Windows (PowerShell):
     - `py -3.11 -m venv .venv`
     - `..\.venv\Scripts\Activate`
   - Linux/macOS:
     - `python3.11 -m venv .venv`
     - `source .venv/bin/activate`
3. Установить зависимости:
   - `python -m pip install -U pip`
   - `pip install -r requirements.txt`
4. (Опционально) Создать файл .env с параметрами (пример ниже).
5. Запустить API:
   - Для разработки (автоперезапуск): 
     - `uvicorn app.main:app --reload --host 0.0.0.0 --port 8000`
   - Обычный режим: 
     - `uvicorn app.main:app --host 0.0.0.0 --port 8000`
6. Проверить работу:
   - Открыть http://127.0.0.1:8000/docs
   - Или: `curl http://127.0.0.1:8000/health`

## Переменные окружения
Параметры вынесены в настройки Pydantic. Источники:
- переменные окружения;
- файл .env в корне проекта;
- docker-compose.yml.

Ключевые параметры и значения по умолчанию:
- MODEL_PATH: путь к ONNX‑модели. По умолчанию `./models/model.onnx` (локально) или `/models/model.onnx` (в Docker по compose).
- DEVICE: `cpu` или `cuda:0` (при наличии GPU-провайдера). По умолчанию `cpu`.
- CONF_THRESH: порог доверия, по умолчанию `0.25`.
- IOU_THRESH: порог IoU для NMS, по умолчанию `0.5`.
- MAX_IMAGE_MB: максимальный размер загружаемого изображения (МБ), по умолчанию `100`.
- ALLOWED_FORMATS: допустимые форматы, по умолчанию `{"JPEG","PNG","BMP","WEBP"}`.
- CORS_ALLOW_*: параметры CORS (по умолчанию открыты для удобства тестирования).

Пример .env:
```
MODEL_PATH=./models/model.onnx
DEVICE=cpu
CONF_THRESH=0.25
IOU_THRESH=0.5
MAX_IMAGE_MB=100
ALLOWED_FORMATS=JPEG,PNG,BMP,WEBP
```

## Эндпойнты и проверка
- Корень сервиса:
  - GET `/` → краткая справка и список эндпойнтов.
- Проверка здоровья:
  - GET `/health` → `{ "status": "ok" }`.
- Детекция логотипов:
  - POST `/detect` (multipart/form-data) с полем `file`.

Примеры запросов:
- curl (Windows PowerShell):
  - `curl -Method POST http://127.0.0.1:8000/detect -Form file=@"C:\\path\\to\\image.jpg"`
- curl (Linux/macOS):
  - `curl -X POST http://127.0.0.1:8000/detect -F file=@/path/to/image.jpg`

Пример ответа:
```
{
  "detections": [
    { "bbox": { "x_min": 123, "y_min": 45, "x_max": 456, "y_max": 300 } }
  ]
}
```
При отсутствии детекций: `{"detections": []}`.

## Частые ошибки и способы решения
- Модель не найдена: проверьте MODEL_PATH и наличие файла модели (в Docker — корректность примонтированного тома `./models:/models:ro`).
- Слишком большой файл (413): уменьшите изображение или увеличьте `MAX_IMAGE_MB`.
- Неподдерживаемый формат (400): допустимые форматы задаются `ALLOWED_FORMATS`.
- Ошибки валидации 422: убедитесь, что поле запроса называется `file` и отправляется как multipart/form-data.
- GPU не используется: установите onnxruntime-gpu, задайте `DEVICE=cuda:0`, для Docker добавьте `--gpus all` и используйте образ, собранный с onnxruntime-gpu.

# Подход к решению

В этой части описан мой подход к решению задачи с обоснованием выбора тех или иных инструментов. В последней секции описаны возможные улучшения и заключения по работе над проектом.

## Подготовка и выбор модели

При первоначальной оценке сложности предстоящей работы, я определился со стэком технологий и провел сравнительный анализ. Конкретно для ручной и затем полуавтоматической разметки данных я выбрал open-source приложение [Label Studio](https://github.com/HumanSignal/label-studio/) в силу положительных отзывов, удобства работы и перспектив подключения своей модели для предразметки изображений. Затем я изучил текущие state-of-the-art модели для детекции объектов на изображении и остановился на линейке [YOLO](https://docs.ultralytics.com/ru/models/), с которыми я имею опыт работы и которые проверены временем. Из них я решил провести сравнение моделей v11 и v12, чтобы выбрать наиболее подходящую под решение задачи.

Следующим шагом я настроил пайплайн работы: изначально разметку я проводил вручную (первую 1000 изображений), затем собирал размеченные данные в датасет (800/200 обучение/валидация) при помощи простых самописных скриптов на Python, сохраняющих пропорцию изображений с логотипами и без них, и производил обучение YOLO в Google Colab. Затем модель я запускал уже локально и через [Label Studio ML Backend](https://github.com/HumanSignal/label-studio-ml-backend) и она производила предварительную разметку, которую я поправлял или принимал. 
Первая итерация обучения показала высокую точность обеих моделей, логотип детектировался практически идеально в различных интерфейсах, но иногда пропускался в других окружениях (3D модель, фотография, редкое цветовое сочетание). Тем не менее это был хороший результат для всего 1/30 данных. Из четырех моделей выбор пал на YOLOv11n, как более стабильную и эффективную модель, показавшую лучшие результаты по итогу обучения и учитывая результаты [исследования](https://arxiv.org/abs/2407.12040v7). 

Полные параметры обучения (они же будут использоваться далее для дообучения):
- Эпохи: **50**
- Patience: **10** эпох
- Приводимый размер изображения: **800x800** (подобрано эмпирически)
- Batch: **0.8** (рассчитывается так, чтобы при обучении использовалось 80% свободной памяти GPU)
- GPU: **T4**

Результаты сравнения v11 и v12 на валидации:

| Модель   | Box Precision | Box Recall | Box mAP50 | Box mAP50-95 | Среднее время обучения 1 эпохи |
|----------|---------------|------------|-----------|--------------|--------------------------------|
| YOLOv11n | **0.996**     | 0.926      | **0.963** | **0.910**    | **~16 сек.**                   |
| YOLOv11s | 0.991         | 0.889      | 0.952     | 0.871        | ~22 сек.                       |
| YOLOv12n | 0.961         | 0.921      | 0.960     | 0.883        | ~23 сек.                       |
| YOLOv12s | 0.925         | **0.955**  | 0.889     | 0.820        | ~42 сек.                       |

## Обучение модели

Далее я решил модифицировать пайплайн и упростить работу. Для этого я арендовал облачный сервер через [Selectel](https://selectel.ru), где имел опыт работы с различными моделями, со следующими характеристиками:

- GPU: **T4**
- vCPU: **8 ядер**
- RAM: **16 ГБ** (+4 ГБ swap памяти для полного обучения)
- SSD: **45 ГБ, 200 МБ/с, 7000/4000 IOPS**
- OS: **Ubuntu 22.04 LTS 64-bit** (с предустановленным ПО для работы с моделями, в том числе CUDA 12.2, Python 3.10 и GPU Drivers 570)

На сервере я развернул инструменты и финализировал пайплайн:

1. Разметка данных
2. Экспорт меток, сбор изображений в датасет
3. Деление датасета на обучающую и валидационную части (с сохранением пропорции количества пустых изображений и логотипов)
4. Дообучение модели предыдущей итерации, фиксация результатов
5. Экспорт весов, интеграция обновленной модели в платформу разметки

Всего было 4 такие итерации (если учитывать изначальную, до аренды сервера). Параллельно с ними я проводил сравнения моделей, модифицировал скрипты для автоматизации большинства пунктов, оптимизировал параметры обучения и писал сервис на Python. Больше всего времени ушло на разметку данных, хотя с каждой итерацией она проходила всё проще и требовала меньше ручной коррекции.

В датасет также было добавлено несколько сделанных мной фотографий из университета, где я очень просто моделировал разные повороты подушек с логотипом и изменял освещение. Вероятно, они существенно не повлияли на обучение и работу модели из-за небольшого количества, но после дообучения с ними она стала лучше воспринимать трехмерные модели логотипа Т-Банка и их перевернутые вариации.

На последней итерации я снизил `Patience` до 5 эпох, а общее количество до 15 эпох, так как модель уже была хорошо обучена и рисковала переобучиться.

В таблице приведены промежуточные результаты обучения YOLOv11n на валидационной выборке на разных итерациях работы:

| Итерация | Размер данных (количество изображений с логотипом / без) | Box Precision | Box Recall | Box mAP50 | Box mAP50-95 | Среднее время обучения 1 эпохи |
|----------|----------------------------------------------------------|---------------|------------|-----------|--------------|--------------------------------|
| **1**    | **1000** (129 / 871) изображений                         | 0.996         | 0.926      | 0.963     | 0.910        | ~16 сек.                       |
| **2**    | **4520** (558 / 3962) изображений                        | 0.967         | 0.912      | 0.971     | 0.885        | ~40 сек.                       |
| **3**    | **20026** (2296 / 17730) изображений                     | 0.965         | 0.968      | 0.990     | 0.937        | ~250 сек.                      |
| **4**    | **30100** (3485 / 26615) изображений                     |               |            |           |              | ~370 сек.                      |

Можно заметить, что с ростом данных модель несколько улучшала некоторые метрики и набирала большую "обобщающую способность", хотя другие метрики при этом снижались. Подробнее об этом я рассуждаю в пункте [Возможные улучшения и анализ работы](#возможные-улучшения-и-анализ-работы).

В целом YOLOv11n показала хороший результат и уверенно обучается. Рост времени обучения затруднял эксперименты с целью добиться еще большей эффективности, тем не менее был достигнут неплохой результат, по моему мнению.

## Разработка сервиса

В основу сервиса легли библиотеки FastAPI, Uvicorn, ONNX Runtime и Pydantic. Так как ранее я имел более обширный опыт разработки сервисов на Java, то большая часть времени ушла на изучение библиотек и других "эталонных" проектов со схожим стэком для определения лучших практик. Сервис разбит на разные модули небольшого размера для удобства работы и оценки. Конечно, можно было написать всё гораздо компактнее, но я считаю что одна из важных составляющих каждого проекта -- возможность масштабирования без больших затрат и грамотное проектирование архитектуры.

Изначально сервис имел прямую зависимость от библиотеки Ultralytics для запуска модели YOLO, что вызвало трудности при сборке так как она требует загрузки большого числа зависимостей и при этом избыточна. Затем было принято решение перейти на формат [ONNX](https://onnx.ai/) для большей гипотетической масштабируемости (можно заменить модель YOLO на другую) и скорости инференса. Это решение позволило полностью отказаться от GPU и производить детекции только на CPU за очень короткое время. Сборка допускает использование GPU (инструкция есть выше), но это не обязательно для работы текущей модели, что делает итоговый сервис более универсальным и существенно менее требовательным к ресурсам. Итоговое решение включает в себя сам REST API сервис и вспомогательные модули для инференса и работы с моделью, так как из-за формата ONNX необходимо вручную обрабатывать изображение до и после детекции. 

Для проверки корректности прогнозов модели так же были написаны вспомогательные скрипты для отрисовки предсказания и автоматической проверки эндпойнтов и формата ошибок. Они не включены в этот репозиторий по причине того, что были нужны только на некоторых этапах разработки.

## Возможные улучшения и анализ работы

Как итог я узнал и попробовал много нового для себя как со стороны настройки пайплайна обучения модели, так и разработки REST API сервиса на Python. Безусловно, многие ошибки в процессе работы привели к тому, что у меня оставалось не так много времени на эксперименты с моделью и работу с данными, но благодаря им я стал глубже понимать более серьезные аспекты работы с продуктами на основе машинного обучения, так как ранее чаще имел дело с обучающими проектами.

Анализируя прошедшую работу я могу выделить несколько моментов, в которых можно улучшить как модель так и всё решение в целом:

- Стоило уделить больше времени на первичный анализ данных, составление собственного датасета (вероятно искусственного, на основе наложения логотипа на разные фоны, поворотов, искажений и наложений разных фильтров на изображение). Возможно так же стоило использовать другой подход к разметке первой части датасета, например использовать zero-shot подходы (как верно было указано в подсказках). Могу предположить, что более качественный подход к работе с данными мог привести к существенно лучшим результатам в работе модели и сократить время разработки и, особенно, разметки данных.


- Определенно некоторая часть ошибок модели связана не с недостатком обучения (я наоборот считаю, что может иметь место даже небольшое переобучение) или выбором архитектуры модели, а с человеческим фактором во время разметки. Могу привести пример: датасет содержит много повторяющихся изображений и их частей, отличающихся разрешением и удаленными сегментами, поэтому во время ручной разметки двух идентичных логотипов с одного исходного изображения так или иначе допускается неточность и метки не совпадают идеально. Разница даже в несколько пикселей приводит к тому, что модель просто не может найти идеально решение: если идеально предсказывать одну метку, то ошибка будет возникать из-за ее различия со второй и наоборот. И так как изображений много, такие ошибки копятся и вынуждают модель находить "компромисс" и предсказывать что-то среднее. 

    Другой пример возникновения таких ошибок это "быстрая разметка" на основе предсказаний самой же модели. После 1 итерации обучения для ускорения работы модель использовалась для предразметки изображений в датасете. И, хотя они проходили ручной контроль, ошибки в большой долей вероятности были.
    
    Я полагаю, что именно эти накопленные ошибки и приводят к тому, что с каждой итерацией метрики немного варьируются в ту или иную сторону.


- Так как ограничения из ТЗ позволяли, стоило попробовать так же и более сложные архитектуры, которые требуют GPU для быстрого инференса. Обучение модели можно было произвести и на более мощной GPU, чем T4, хотя для большей честности я выбрал именно ее. Выбор другой модели мог привести к лучшим результатам с одной стороны, а с другой возможно что для этой (относительно несложной задачи для модели) сильные архитектуры будут избыточны.


- В дополнение к предыдущему пункту можно добавить, что для большей уверенности и точности сравнений моделей ([Подготовка и выбор модели](#подготовка-и-выбор-модели)) правильно было бы обучать каждую не 2-3 раза, а более 10 и попробовать их на разных размерах данных. Нельзя исключать, что неэффективность версии small и семейства v12 свойственна только маленьким данным и на большем масштабе они могут показать лучший результат.

## Список использованных инструментов:
- [Label Studio](https://github.com/HumanSignal/label-studio/)
- [Label Studio ML Backend](https://github.com/HumanSignal/label-studio-ml-backend)
- [Google Colab](https://colab.research.google.com/)
- [Selectel](https://selectel.ru)
- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- [Ultralytics YOLO](https://docs.ultralytics.com/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [Uvicorn](https://www.uvicorn.org/)
- [Pydantic](https://pydantic-docs.helpmanual.io/)
- [ONNX Runtime](https://onnxruntime.ai/)

# Благодарности

Хочу выразить благодарность всем, кто поддерживал меня во время долгих разметок и постоянных ошибок. Отдельное спасибо моему отцу помощь с разметкой, которая помогала мне находить время для учебы в университете.
